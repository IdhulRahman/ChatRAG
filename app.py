import os
from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from config.llm_conf import create_llm
from config.doc_Loader import load_documents
from config.embed_model import create_embedding_model, create_and_save_optimum_model
from config.vector_index import create_index
from config.query_engine import setup_query_engine
from llama_index.core import (
    StorageContext,
    load_index_from_storage,
    Settings
)

# Initialize FastAPI app
app = FastAPI()

# Mount static files for CSS and JavaScript
app.mount("/static", StaticFiles(directory="static"), name="static")

# Setup template directory
templates = Jinja2Templates(directory="templates")

# Define request body model
class ChatRequest(BaseModel):
    user_input: str

# Function to initialize the chatbot engine
def initialize_chatbot():
    try:
        embedding_folder = "mxbai-rerank"
        if os.path.exists(embedding_folder):
            embed_model = create_embedding_model()
        else:
            create_and_save_optimum_model()
            embed_model = create_embedding_model()
        
        Settings.embed_model = embed_model

        llm = create_llm()
        Settings.llm = llm

        index_folder = "datavector"
        if not os.path.exists(index_folder):
            # Load the documents from the dataset
            documents = load_documents(["dataset/Data3.txt"])
            if not documents:
                raise ValueError("Documents could not be loaded from dataset/Data3.txt")

            # Create the index from documents and embedding model
            index = create_index(documents, embed_model)
        else:
            # Load the index from storage
            storage_context = StorageContext.from_defaults(persist_dir=index_folder)
            index = load_index_from_storage(storage_context, index_id="vector_index")
        
        query_engine = setup_query_engine(index, llm)
        return query_engine

    except Exception as e:
        print(f"Error during chatbot initialization: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to initialize chatbot: {e}")

# Initialize chatbot engine when app starts
query_engine = initialize_chatbot()

# Function to handle chatbot response
def chat_with_bot(user_input):
    try:
        response = query_engine.query(user_input)

        # Handle response streaming or direct response
        response_text = ""
        if hasattr(response, "response_stream"):
            for chunk in response.response_stream:
                response_text += chunk
        else:
            response_text = str(response)

        if not response_text:
            raise ValueError("No response generated by the chatbot")

        return response_text

    except Exception as e:
        print(f"Error during chatbot query: {e}")
        return "Sorry, an error occurred while processing your request."

# POST endpoint to interact with the chatbot
@app.post("/chat")
async def chat(request: ChatRequest):
    user_input = request.user_input
    response = chat_with_bot(user_input)
    return {"user_input": user_input, "response": response}

# GET endpoint to render the HTML template (homepage)
@app.get("/")
async def root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})